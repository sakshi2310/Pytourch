{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f08283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda9ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"100_Unique_QA_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a17328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Who directed the movie 'Titanic'?</td>\n",
       "      <td>JamesCameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Which superhero is also known as the Dark Knight?</td>\n",
       "      <td>Batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>What is the capital of Brazil?</td>\n",
       "      <td>Brasilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Which fruit is known as the king of fruits?</td>\n",
       "      <td>Mango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Which country is known for the Eiffel Tower?</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question        answer\n",
       "0                      What is the capital of France?         Paris\n",
       "1                     What is the capital of Germany?        Berlin\n",
       "2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
       "3     What is the largest planet in our solar system?       Jupiter\n",
       "4      What is the boiling point of water in Celsius?           100\n",
       "..                                                ...           ...\n",
       "85                  Who directed the movie 'Titanic'?  JamesCameron\n",
       "86  Which superhero is also known as the Dark Knight?        Batman\n",
       "87                     What is the capital of Brazil?      Brasilia\n",
       "88        Which fruit is known as the king of fruits?         Mango\n",
       "89       Which country is known for the Eiffel Tower?        France\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac593f5b",
   "metadata": {},
   "source": [
    "## Tokennize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45b8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace(\"'\",'')\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc942dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833ea5c",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d18fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'<UNK>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45f58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_vocab(row):\n",
    "    tokenize_question = tokenize(row['question'])\n",
    "    tokenzie_answer = tokenize(row['answer'])\n",
    "    merged_tokens = tokenize_question + tokenzie_answer\n",
    "    print(merged_tokens)\n",
    "\n",
    "    for token in merged_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    # print(tokenize_question,tokenzie_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d34837d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
      "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
      "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
      "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
      "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
      "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
      "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
      "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
      "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
      "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
      "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
      "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
      "['who', 'discovered', 'gravity', 'newton']\n",
      "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
      "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
      "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
      "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
      "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
      "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
      "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
      "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
      "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
      "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
      "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
      "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
      "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
      "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
      "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
      "['who', 'painted', 'starry', 'night', 'vangogh']\n",
      "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
      "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
      "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
      "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
      "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
      "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
      "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
      "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
      "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
      "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
      "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
      "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
      "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
      "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
      "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
      "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
      "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
      "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
      "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
      "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
      "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
      "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
      "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
      "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
      "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
      "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
      "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
      "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
      "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
      "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
      "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
      "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
      "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
      "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
      "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
      "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
      "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
      "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
      "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
      "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
      "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
      "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
      "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
      "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
      "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
      "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
      "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
      "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
      "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
      "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
      "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
      "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
      "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
      "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(bulid_vocab,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e49e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'what': 1, 'is': 2, 'the': 3, 'capital': 4, 'of': 5, 'france': 6, 'paris': 7, 'germany': 8, 'berlin': 9, 'who': 10, 'wrote': 11, 'to': 12, 'kill': 13, 'a': 14, 'mockingbird': 15, 'harper-lee': 16, 'largest': 17, 'planet': 18, 'in': 19, 'our': 20, 'solar': 21, 'system': 22, 'jupiter': 23, 'boiling': 24, 'point': 25, 'water': 26, 'celsius': 27, '100': 28, 'painted': 29, 'mona': 30, 'lisa': 31, 'leonardo-da-vinci': 32, 'square': 33, 'root': 34, '64': 35, '8': 36, 'chemical': 37, 'symbol': 38, 'for': 39, 'gold': 40, 'au': 41, 'which': 42, 'year': 43, 'did': 44, 'world': 45, 'war': 46, 'ii': 47, 'end': 48, '1945': 49, 'longest': 50, 'river': 51, 'nile': 52, 'japan': 53, 'tokyo': 54, 'developed': 55, 'theory': 56, 'relativity': 57, 'albert-einstein': 58, 'freezing': 59, 'fahrenheit': 60, '32': 61, 'known': 62, 'as': 63, 'red': 64, 'mars': 65, 'author': 66, '1984': 67, 'george-orwell': 68, 'currency': 69, 'united': 70, 'kingdom': 71, 'pound': 72, 'india': 73, 'delhi': 74, 'discovered': 75, 'gravity': 76, 'newton': 77, 'how': 78, 'many': 79, 'continents': 80, 'are': 81, 'there': 82, 'on': 83, 'earth': 84, '7': 85, 'gas': 86, 'do': 87, 'plants': 88, 'use': 89, 'photosynthesis': 90, 'co2': 91, 'smallest': 92, 'prime': 93, 'number': 94, '2': 95, 'invented': 96, 'telephone': 97, 'alexander-graham-bell': 98, 'australia': 99, 'canberra': 100, 'ocean': 101, 'pacific-ocean': 102, 'speed': 103, 'light': 104, 'vacuum': 105, '299,792,458m/s': 106, 'language': 107, 'spoken': 108, 'brazil': 109, 'portuguese': 110, 'penicillin': 111, 'alexander-fleming': 112, 'canada': 113, 'ottawa': 114, 'mammal': 115, 'whale': 116, 'element': 117, 'has': 118, 'atomic': 119, '1': 120, 'hydrogen': 121, 'tallest': 122, 'mountain': 123, 'everest': 124, 'city': 125, 'big': 126, 'apple': 127, 'newyork': 128, 'planets': 129, 'starry': 130, 'night': 131, 'vangogh': 132, 'formula': 133, 'h2o': 134, 'italy': 135, 'rome': 136, 'country': 137, 'famous': 138, 'sushi': 139, 'was': 140, 'first': 141, 'person': 142, 'step': 143, 'moon': 144, 'armstrong': 145, 'main': 146, 'ingredient': 147, 'guacamole': 148, 'avocado': 149, 'sides': 150, 'does': 151, 'hexagon': 152, 'have': 153, '6': 154, 'china': 155, 'yuan': 156, 'pride': 157, 'and': 158, 'prejudice': 159, 'jane-austen': 160, 'iron': 161, 'fe': 162, 'hardest': 163, 'natural': 164, 'substance': 165, 'diamond': 166, 'continent': 167, 'by': 168, 'area': 169, 'asia': 170, 'president': 171, 'states': 172, 'george-washington': 173, 'bird': 174, 'its': 175, 'ability': 176, 'mimic': 177, 'sounds': 178, 'parrot': 179, 'longest-running': 180, 'animated': 181, 'tv': 182, 'show': 183, 'simpsons': 184, 'vaticancity': 185, 'most': 186, 'moons': 187, 'saturn': 188, 'romeo': 189, 'juliet': 190, 'shakespeare': 191, 'earths': 192, 'atmosphere': 193, 'nitrogen': 194, 'bones': 195, 'adult': 196, 'human': 197, 'body': 198, '206': 199, 'metal': 200, 'liquid': 201, 'at': 202, 'room': 203, 'temperature': 204, 'mercury': 205, 'russia': 206, 'moscow': 207, 'electricity': 208, 'benjamin-franklin': 209, 'second-largest': 210, 'land': 211, 'color': 212, 'ripe': 213, 'banana': 214, 'yellow': 215, 'month': 216, '28': 217, 'days': 218, 'common': 219, 'february': 220, 'study': 221, 'living': 222, 'organisms': 223, 'called': 224, 'biology': 225, 'home': 226, 'great': 227, 'wall': 228, 'bees': 229, 'collect': 230, 'from': 231, 'flowers': 232, 'nectar': 233, 'opposite': 234, 'day': 235, 'south': 236, 'korea': 237, 'seoul': 238, 'bulb': 239, 'edison': 240, 'humans': 241, 'breathe': 242, 'survival': 243, 'oxygen': 244, '144': 245, '12': 246, 'pyramids': 247, 'giza': 248, 'egypt': 249, 'sea': 250, 'creature': 251, 'eight': 252, 'arms': 253, 'octopus': 254, 'holiday': 255, 'celebrated': 256, 'december': 257, '25': 258, 'christmas': 259, 'yen': 260, 'legs': 261, 'spider': 262, 'sport': 263, 'uses': 264, 'net,': 265, 'ball,': 266, 'hoop': 267, 'basketball': 268, 'kangaroos': 269, 'female': 270, 'minister': 271, 'uk': 272, 'margaretthatcher': 273, 'fastest': 274, 'animal': 275, 'cheetah': 276, 'periodic': 277, 'table': 278, 'spain': 279, 'madrid': 280, 'closest': 281, 'sun': 282, 'father': 283, 'computers': 284, 'charlesbabbage': 285, 'mexico': 286, 'mexicocity': 287, 'colors': 288, 'rainbow': 289, 'musical': 290, 'instrument': 291, 'black': 292, 'white': 293, 'keys': 294, 'piano': 295, 'americas': 296, '1492': 297, 'christophercolumbus': 298, 'disney': 299, 'character': 300, 'long': 301, 'nose': 302, 'grows': 303, 'it': 304, 'when': 305, 'lying': 306, 'pinocchio': 307, 'directed': 308, 'movie': 309, 'titanic': 310, 'jamescameron': 311, 'superhero': 312, 'also': 313, 'dark': 314, 'knight': 315, 'batman': 316, 'brasilia': 317, 'fruit': 318, 'king': 319, 'fruits': 320, 'mango': 321, 'eiffel': 322, 'tower': 323}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ccb20",
   "metadata": {},
   "source": [
    "## Convert the words to numerical index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "717501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the words to numerical indices\n",
    "\n",
    "def text_to_indices(text,vocab):\n",
    "    indexed_text = []\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "        else:\n",
    "            indexed_text.append(vocab['<UNK>'])\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206c9fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"What is this\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bf92018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "209d96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self,df,vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        numerical_question = text_to_indices(self.df.iloc[index]['question'],self.vocab)\n",
    "        numerical_answer = text_to_indices(self.df.iloc[index]['answer'],self.vocab)\n",
    "\n",
    "        return torch.tensor(numerical_question),torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4472b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297e9e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5, 6]), tensor([7]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2af70e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader = DataLoader(dataset,batch_size=1,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61a1a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n"
     ]
    }
   ],
   "source": [
    "for question,answer in DataLoader:\n",
    "    print(question,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2d38952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4364f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim=50)\n",
    "        self.rnn = nn.RNN(50,64,batch_first=True)\n",
    "        self.fc = nn.Linear(64,vocab_size)\n",
    "\n",
    "    def forward(self,question):\n",
    "        embadded_question =  self.embedding(question)\n",
    "        hideen_state ,final_output = self.rnn(embadded_question)\n",
    "        output = self.fc(final_output.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ffc8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "122236b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "642ec330",
   "metadata": {},
   "outputs": [],
   "source": [
    "cirtertion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1e02c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :1 , Loss : 521.7672834396362\n",
      "Epoch :2 , Loss : 455.27131748199463\n",
      "Epoch :3 , Loss : 379.1168894767761\n",
      "Epoch :4 , Loss : 316.01873087882996\n",
      "Epoch :5 , Loss : 263.05425548553467\n",
      "Epoch :6 , Loss : 214.00405550003052\n",
      "Epoch :7 , Loss : 169.62807834148407\n",
      "Epoch :8 , Loss : 131.17900997400284\n",
      "Epoch :9 , Loss : 99.82066175341606\n",
      "Epoch :10 , Loss : 75.81815725564957\n",
      "Epoch :11 , Loss : 57.6555519849062\n",
      "Epoch :12 , Loss : 44.60003152489662\n",
      "Epoch :13 , Loss : 34.8943462818861\n",
      "Epoch :14 , Loss : 28.190353855490685\n",
      "Epoch :15 , Loss : 23.010728985071182\n",
      "Epoch :16 , Loss : 19.223744496703148\n",
      "Epoch :17 , Loss : 16.15590975433588\n",
      "Epoch :18 , Loss : 13.84206361323595\n",
      "Epoch :19 , Loss : 11.855203241109848\n",
      "Epoch :20 , Loss : 10.266066789627075\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for question , answer in DataLoader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(question)\n",
    "        # print(output.shape)\n",
    "        # loss  \n",
    "        loss = cirtertion(output,answer[0])\n",
    "\n",
    "        # gradients \n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    print(f\"Epoch :{epoch+1} , Loss : {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3352d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,question,threshhold=0.5):\n",
    "\n",
    "    # convert question to numeric\n",
    "    numerical_question = text_to_indices(question,vocab)\n",
    "    # print(numerical_question)\n",
    "    # tensor \n",
    "    question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "    # print(question_tensor)\n",
    "\n",
    "    # send to model\n",
    "    output = model(question_tensor)\n",
    "\n",
    "    # convet logits to probs\n",
    "    probs = torch.nn.functional.softmax(output,dim=1)\n",
    "\n",
    "    # find the max probs \n",
    "    value , index = torch.max(probs,dim=1)\n",
    "    print(value)\n",
    "    if value<threshhold:\n",
    "        print(\"I don't know\")\n",
    "    \n",
    "    print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "97ef15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5477], grad_fn=<MaxBackward0>)\n",
      "paris\n"
     ]
    }
   ],
   "source": [
    "predict(model , \"what is the captital of france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee330d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2edab3c5",
   "metadata": {},
   "source": [
    "## Model Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40098395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11a1bd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(324, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.Embedding(324,embedding_dim=50)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcb3a045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5539e-01,  8.0644e-01, -1.3124e+00, -1.9226e-01,  4.1745e-01,\n",
       "          4.4776e-01, -1.2109e-01, -1.9486e-01,  6.4431e-01,  1.5087e+00,\n",
       "          2.7660e-01, -5.9878e-01,  1.1756e+00, -6.4748e-01, -2.0304e+00,\n",
       "         -1.2889e-01, -1.2045e+00,  1.1808e+00,  2.3235e-01, -7.2151e-01,\n",
       "          2.0419e+00, -2.8482e+00,  1.3036e+00, -1.3826e+00, -1.1541e+00,\n",
       "          4.2362e-02, -1.0344e-02, -1.3030e+00,  2.5148e+00, -5.7832e-01,\n",
       "         -5.1541e-01, -3.3687e-01,  7.4102e-01, -1.0479e+00, -1.1568e-01,\n",
       "         -3.5308e-01,  8.7438e-01, -2.0649e+00, -1.2401e+00, -1.7648e+00,\n",
       "          3.4438e-01,  4.0180e-01,  4.5518e-01,  1.6867e-04,  9.5165e-01,\n",
       "          6.8536e-01,  8.6587e-01, -8.5315e-01,  1.6526e+00, -2.9678e-01],\n",
       "        [ 1.1449e-01,  1.1872e+00, -6.2184e-01, -2.3275e+00, -1.2257e-01,\n",
       "         -2.4952e+00,  1.0248e+00,  1.4591e+00,  1.0074e+00,  2.6644e-01,\n",
       "          1.9978e-01,  1.1186e+00, -8.1435e-01, -1.0156e+00, -3.1636e-01,\n",
       "          2.4951e-01,  6.6935e-02, -1.3017e+00, -4.5342e-01,  2.8241e+00,\n",
       "         -7.2358e-01,  1.0450e+00,  4.3076e-03, -1.5178e+00,  1.5173e+00,\n",
       "         -7.6033e-02,  2.6278e-01, -7.7459e-01, -8.7623e-01, -1.0985e+00,\n",
       "         -5.9755e-01,  2.0815e-01, -1.5231e+00, -8.7077e-01, -9.8999e-01,\n",
       "         -3.7732e-01,  1.8841e+00, -2.1633e+00, -6.0671e-01, -1.5591e+00,\n",
       "          4.5751e-01,  1.1376e-01, -2.1344e-01,  9.0639e-02,  9.3374e-01,\n",
       "          7.2003e-01,  9.4754e-01, -9.3761e-01, -1.8296e+00, -2.3003e+00],\n",
       "        [ 1.0109e+00,  1.4676e-01,  2.2468e-01, -1.7343e+00,  7.9159e-01,\n",
       "          2.7206e-01,  1.6834e-01,  1.8643e+00, -2.9784e-01, -3.9323e-01,\n",
       "          1.4440e+00, -6.5632e-01,  5.5201e-01, -1.9122e+00,  3.0880e-01,\n",
       "          4.5897e-01, -1.2405e+00, -9.4751e-01,  2.5802e-01,  3.3305e-01,\n",
       "          7.0396e-02, -2.0625e-01,  4.2009e-03, -9.5172e-01,  2.2541e-01,\n",
       "          7.7116e-01,  2.4676e+00, -1.0261e+00,  1.0847e+00, -1.5191e+00,\n",
       "         -1.6770e-01, -1.6160e+00,  1.3848e+00,  1.0960e-02,  1.6662e-02,\n",
       "          5.3595e-01, -9.9497e-01, -5.2351e-01,  6.6161e-01, -1.8972e+00,\n",
       "          1.3566e+00,  4.3758e-01,  5.5931e-02, -4.3127e-01, -7.7110e-01,\n",
       "         -1.4408e-01, -6.5772e-01,  5.4466e-01, -7.5237e-01, -1.7968e-01],\n",
       "        [ 1.1336e+00,  1.0980e-01, -1.0226e-01, -5.1038e-01, -1.6692e-02,\n",
       "         -2.8310e-02, -9.3147e-01,  1.0434e+00, -3.3315e-01, -2.1735e-01,\n",
       "         -5.0226e-02, -3.3073e-02,  6.5467e-01, -1.2898e-02, -1.5873e+00,\n",
       "         -4.3565e-01,  1.0038e+00,  3.2619e-01, -1.1466e+00, -1.8212e-01,\n",
       "          1.8910e-01, -1.8372e+00, -6.2492e-01,  9.6732e-01,  8.0560e-01,\n",
       "          9.2703e-01, -1.3694e+00, -5.6342e-01,  5.8576e-01, -6.0980e-01,\n",
       "         -8.3645e-01,  6.8940e-01,  1.8679e+00, -8.3099e-01,  3.6475e-01,\n",
       "         -5.5161e-01, -4.1982e-01, -1.0274e+00,  5.5828e-01, -1.4580e+00,\n",
       "          4.9202e-01, -9.5052e-01,  8.7478e-01, -3.1269e-01, -1.2241e+00,\n",
       "         -1.6210e+00,  1.5935e+00,  4.9700e-01, -4.1178e-01, -7.5957e-01],\n",
       "        [ 1.8632e-01, -2.0735e-01,  1.8373e+00,  4.0579e-01,  1.3607e+00,\n",
       "          6.5423e-02,  4.2395e-01, -1.2229e+00, -5.9445e-01, -4.9971e-01,\n",
       "          1.3866e-01, -2.1311e-01,  3.0188e-01,  1.6402e-01, -7.8781e-01,\n",
       "         -4.1716e-01, -7.3806e-01, -7.5082e-01,  6.8383e-01, -4.9607e-01,\n",
       "          4.6763e-01, -5.5124e-01,  9.5459e-01,  2.6520e+00,  1.8496e-01,\n",
       "         -4.1485e-01, -2.6488e-01,  4.3581e-01,  8.4050e-01, -6.2493e-01,\n",
       "          6.7377e-01, -8.3558e-01,  2.0354e-01,  9.4705e-01,  5.0510e-01,\n",
       "          1.3016e+00,  5.4620e-01, -9.5059e-01,  6.8644e-01, -5.8397e-01,\n",
       "         -2.4888e+00,  1.0015e+00,  1.7643e-02,  2.6536e-01,  3.2606e-01,\n",
       "         -1.2288e+00, -4.7358e-02,  8.3038e-01,  1.2098e+00,  1.7957e+00],\n",
       "        [-4.0576e-01, -3.5795e-01, -1.0400e+00, -3.0329e-01,  6.3044e-01,\n",
       "         -4.2494e-01, -9.1893e-01,  7.2911e-01,  1.1936e+00,  2.9490e-01,\n",
       "         -1.5613e+00, -7.2455e-01,  4.1678e-01,  1.3328e+00, -6.4859e-01,\n",
       "          3.2034e-01, -3.1473e-01,  5.5267e-01,  8.0203e-01, -1.2678e-02,\n",
       "         -3.6021e-01, -3.2750e-01,  5.1073e-01, -1.3183e+00, -7.3407e-01,\n",
       "         -2.2221e-02,  4.7043e-01,  1.7981e-01, -1.4106e-01,  3.2388e+00,\n",
       "          6.2389e-01,  9.4654e-01,  1.8047e+00,  9.7414e-01, -7.6074e-01,\n",
       "         -9.7209e-01, -8.0100e-01, -8.0825e-01,  6.7267e-01, -9.2361e-01,\n",
       "          9.5710e-03,  1.1711e+00,  8.1239e-01,  8.8205e-01,  1.6948e+00,\n",
       "          8.3656e-01,  9.3357e-01, -2.2429e+00,  1.7786e-01,  1.8019e+00]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x(dataset[0][0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "088ac5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nn.RNN(50,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6763982d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5713, -0.1468, -0.0578,  0.1772, -0.4891,  0.3963,  0.1013, -0.3394,\n",
       "          -0.3546,  0.6813, -0.5481,  0.3044, -0.3667,  0.0095,  0.4669, -0.1087,\n",
       "           0.1114, -0.5379, -0.0802, -0.4273,  0.4638,  0.4010,  0.3501,  0.0519,\n",
       "          -0.5569,  0.3172, -0.3344, -0.3617, -0.0439,  0.1049,  0.1354,  0.3751,\n",
       "          -0.3015,  0.8012, -0.5431, -0.1486,  0.1363, -0.5006,  0.5863,  0.7942,\n",
       "          -0.0355,  0.4526,  0.1344, -0.2949,  0.5729, -0.3690,  0.0108, -0.0689,\n",
       "           0.1197,  0.9092,  0.0289,  0.4153,  0.2494, -0.1558, -0.6656,  0.3849,\n",
       "          -0.2223, -0.3677, -0.6679, -0.8771, -0.5918, -0.2253,  0.5065,  0.4253],\n",
       "         [-0.5796,  0.5715,  0.4406,  0.7288,  0.0516,  0.3404,  0.5540,  0.1703,\n",
       "          -0.3477, -0.5640,  0.5952, -0.6653, -0.4771,  0.6180, -0.0175,  0.8407,\n",
       "           0.2950, -0.6804, -0.2169,  0.4037,  0.7082,  0.7359, -0.0832, -0.0047,\n",
       "           0.6454,  0.6861, -0.5036,  0.6848, -0.4112, -0.5860,  0.6941,  0.3136,\n",
       "          -0.2036, -0.5532, -0.6994,  0.2097,  0.1026,  0.2869, -0.0972,  0.0556,\n",
       "          -0.1657, -0.2772,  0.1108, -0.0254,  0.2888, -0.0276,  0.4974, -0.4755,\n",
       "          -0.8721,  0.2176, -0.7038,  0.5294, -0.9218, -0.5525,  0.1350,  0.0659,\n",
       "          -0.4788, -0.4295, -0.4523, -0.7028, -0.1337,  0.6494,  0.9203,  0.8110],\n",
       "         [-0.3993, -0.1326, -0.4068,  0.4712,  0.3244,  0.2143, -0.2025, -0.6993,\n",
       "          -0.1095,  0.8904,  0.2073,  0.5416, -0.1594,  0.5037, -0.5977,  0.5177,\n",
       "           0.4399,  0.4029, -0.2723, -0.0299,  0.4525,  0.0522,  0.0340, -0.3066,\n",
       "           0.8002, -0.0233,  0.7060,  0.7943,  0.4656,  0.0047,  0.6780, -0.5693,\n",
       "          -0.0664,  0.1898, -0.4521,  0.2681, -0.4838, -0.1853,  0.4518, -0.1368,\n",
       "          -0.5706,  0.4072,  0.0715, -0.3443, -0.6373,  0.0728,  0.3509, -0.2892,\n",
       "          -0.0736, -0.4523, -0.0898,  0.4826,  0.7086,  0.2349, -0.2501,  0.3617,\n",
       "          -0.5622, -0.0323, -0.4346, -0.5864, -0.5448,  0.4807,  0.4526,  0.3414],\n",
       "         [ 0.0600, -0.1222, -0.8085,  0.1255, -0.6678,  0.3919, -0.1137,  0.0658,\n",
       "           0.4154,  0.7238,  0.5397,  0.0588,  0.7723,  0.4723,  0.6179,  0.0877,\n",
       "           0.1263,  0.2511, -0.5180,  0.0870, -0.3784,  0.0934, -0.1607, -0.4803,\n",
       "          -0.2607, -0.7123,  0.4331,  0.1185,  0.4940,  0.0669,  0.4029, -0.0960,\n",
       "          -0.2886,  0.0971, -0.0519,  0.2219,  0.4971,  0.2592, -0.3550,  0.1109,\n",
       "           0.3434,  0.5859, -0.0758, -0.3588,  0.2537,  0.1215, -0.4291, -0.2007,\n",
       "           0.4563, -0.5149,  0.2611, -0.2036,  0.5037,  0.1543,  0.3266,  0.0452,\n",
       "           0.1078, -0.7420, -0.3035, -0.5507, -0.0541, -0.1232,  0.2440,  0.7731],\n",
       "         [ 0.6047, -0.6800, -0.0333, -0.3723, -0.1592,  0.3468,  0.0115, -0.3988,\n",
       "          -0.4169,  0.7565, -0.5085,  0.5982, -0.2084, -0.3435,  0.0415,  0.0986,\n",
       "           0.2841,  0.0567,  0.1234,  0.2002,  0.1305, -0.7586, -0.0106, -0.1863,\n",
       "          -0.6520, -0.1008, -0.0068, -0.6303,  0.4833,  0.7907,  0.5941,  0.4386,\n",
       "           0.5730, -0.4758, -0.0149, -0.3893,  0.4276, -0.3691,  0.4607, -0.0397,\n",
       "           0.6527, -0.1639, -0.0289,  0.5521,  0.0530, -0.1434, -0.3434, -0.1686,\n",
       "           0.0268,  0.2341,  0.8357, -0.2954, -0.2008,  0.8125,  0.4122,  0.1004,\n",
       "           0.0371, -0.8057,  0.3515,  0.2354,  0.3166,  0.1434,  0.4118, -0.3827],\n",
       "         [ 0.3865, -0.5078, -0.2066,  0.2093,  0.1524,  0.3868, -0.5281, -0.2367,\n",
       "          -0.2289,  0.5931, -0.1296, -0.2546,  0.1504,  0.1566, -0.6521,  0.7699,\n",
       "          -0.6697, -0.6909, -0.1644,  0.4750,  0.4642,  0.5522, -0.7572, -0.4761,\n",
       "          -0.3946, -0.0946,  0.3382, -0.3092,  0.1481, -0.1759, -0.0293, -0.2360,\n",
       "          -0.0141, -0.2756, -0.3278,  0.2728,  0.2350,  0.0196,  0.2991,  0.6338,\n",
       "           0.4274, -0.8103,  0.0425, -0.2878,  0.7036, -0.6870,  0.5440,  0.3901,\n",
       "          -0.5520,  0.3850, -0.7780, -0.1039, -0.3356,  0.0933, -0.6044, -0.0773,\n",
       "          -0.1284,  0.5623, -0.4171, -0.0979,  0.0168,  0.5385, -0.1373,  0.0996]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.3865, -0.5078, -0.2066,  0.2093,  0.1524,  0.3868, -0.5281, -0.2367,\n",
       "          -0.2289,  0.5931, -0.1296, -0.2546,  0.1504,  0.1566, -0.6521,  0.7699,\n",
       "          -0.6697, -0.6909, -0.1644,  0.4750,  0.4642,  0.5522, -0.7572, -0.4761,\n",
       "          -0.3946, -0.0946,  0.3382, -0.3092,  0.1481, -0.1759, -0.0293, -0.2360,\n",
       "          -0.0141, -0.2756, -0.3278,  0.2728,  0.2350,  0.0196,  0.2991,  0.6338,\n",
       "           0.4274, -0.8103,  0.0425, -0.2878,  0.7036, -0.6870,  0.5440,  0.3901,\n",
       "          -0.5520,  0.3850, -0.7780, -0.1039, -0.3356,  0.0933, -0.6044, -0.0773,\n",
       "          -0.1284,  0.5623, -0.4171, -0.0979,  0.0168,  0.5385, -0.1373,  0.0996]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c924a242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(a)[0].shape ## Output Layers  ## Hiddent state layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f48deb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b(a)[1].shape # Final Output  # final State Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6411e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9aa072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.Linear(64,324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8497edf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not torch.Size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m z(b)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not torch.Size"
     ]
    }
   ],
   "source": [
    "z(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
